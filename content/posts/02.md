---
title: "Title TODO"
date: 2023-07-01T16:25:59+02:00
---

In the [last post](/posts/six-times-faster-than-c/), we wrote a tiny C program, compiled it, disassembled it,
then tweaked that assembly to make it six times faster. Now we're going to beat it.

<!--more-->

**Disclaimer**: *I'm not an optimization expert, by any means, in fact my
expertise is in high-level, purely-functional languages, where one
doesn't usually think about **how** a program is executed.*

Our first version was able to process 295.26MiB/s, and our best version
reached 1.94GiB/s.

-- TODO check link matches title

The code listings for this post can be found on [Github](https://github.com/414owen/blog-code/tree/master/02-twice-as-fast-as-asm).

So, let's start with the first C version:


{{< tabs groupId="initial" >}}

{{% tab name="C" %}}
```C
int run_switches(char *input) {
  int res = 0;
  while (true) {
    char c = *input++;
    switch (c) {
      case '\0':
        return res;
      case 's':
        res += 1;
        break;
      case 'p':
        res -= 1;
        break;
      default:
        break;
    }
  }
}
```
{{% /tab %}}

{{% tab name="asm + pseudocode" %}}
```asm
# llvm-objdump -d --symbolize-operands --no-addresses --x86-asm-syntax=intel --no-show-raw-insn loop-1-gcc.c.o

run_switches:
      xor     eax, eax            # res = 0
loop:                             # while (true) {
      movsx   ecx, byte ptr [rdi] #   c = *input
      test    ecx, ecx            #   if (c == '\0')
      je      ret                 #     return
      add     rdi, 1              #   input++
      cmp     ecx, 'p'            #   if (c == 'p')
      je      p                   #     goto p
      cmp     ecx, 's'            #   if (c == 's')
      jne     loop                #     continue
      add     eax, 1              #   res++
      jmp     loop                #   continue
p:    add     eax, -1             #   res--
      jmp     loop                # }
ret:  ret
```
{{% /tab %}}

{{% tab name="asm + arrows" %}}
```asm
# objdump -Mintel -d --no-addresses --no-show-raw-insn --visualize-jumps loop-2-gcc.c.o

run_switches:
             xor    eax, eax
loop:
      ╭────➤ movsx  ecx, byte ptr [rdi]
      │      test   ecx, ecx
      │ ╭─── je     ret
      │ │    add    rdi, 1
      │ │    cmp    ecx, 'p'
      │ │ ╭─ je     p
      │ │ │  cmp    ecx, 's'
      ├─│─│─ jne    loop
      │ │ │  add    eax, 1
      ├─│─│─ jmp    loop
p:    │ │ ╰➤ add    eax, -1
      ╰─│─── jmp    loop
ret:    ╰──➤ ret
```
{{% /tab %}}
{{< /tabs >}}

**Runtime:** 3.23s 🐌

**Bitrate**: 295.26MiB/s

## Dropping cases

The way we've expressed this problem is the pretty logical top-down way.
We're looping over out input, one character at a time, and performing a
[case analysis](https://en.wikipedia.org/wiki/Proof_by_exhaustion) over
the possible variants (or switching over the characters, if you prefer).

Based on this case analysis, we run different code.

To avoid this, let's merge the codepaths with common structure.
Both the 'p' and 's' cases add a number to the accumulator, so what if,
instead of performing case analysis to get the next code path, we do so
to get the number to add.

We can look up the number to add in an array:


{{< tabs groupId="arr-lookup" >}}
{{% tab name="C" %}}
```C
#include <stdbool.h>

static
int to_add[256] = {
  ['s'] = 1,
  ['p'] = -1,
};

int run_switches(const char *input) {
  int res = 0;
  while (true) {
    char c = *input++;
    if (c == '\0') {
      return res;
    } else {
      res += to_add[(int) c];
    }
  }
}
```
{{% /tab %}}

{{% tab name="gcc asm" %}}
```asm
# objdump -Mintel -d --no-addresses --no-show-raw-insn --visualize-jumps loop-2-gcc.c.o

run_switches:
           movsx  rax, byte ptr [rdi]
           lea    rdx, [rdi+1]
           xor    ecx, ecx
           lea    rsi, [rip+0]        # <run_switches+0x11>
           test   al,  al
      ╭─── je     ret
      │    nop    dword ptr [rax]
loop: │ ╭➤ add    rdx, 0x1
      │ │  add    ecx, dword ptr [rsi+rax*4]
      │ │  movsx  rax, byte ptr [rdx-1]
      │ │  test   al,  al
      │ ╰─ jne    loop
ret:  ╰──➤ mov    eax, ecx
           ret
```
{{% /tab %}}

{{% tab name="clang asm" %}}
```asm
# objdump -Mintel -d --no-addresses --no-show-raw-insn --visualize-jumps loop-2-clang.c.o

run_switches:
           mov    cl,  byte ptr [rdi]
           test   cl,  cl
      ╭─── je     ret
      │    add    rdi, 0x1
      │    xor    eax, eax
      │    lea    rdx, [rip+0x0]        # <run_switches+0x13>
      │    cs nop word ptr [rax+rax*1+0x0]
      │    nop    dword ptr [rax]
loop: │ ╭➤ movsx  rcx, cl
      │ │  add    eax, dword ptr [rdx+rcx*4]
      │ │  movzx  ecx, byte ptr [rdi]
      │ │  add    rdi, 0x1
      │ │  test   cl,  cl
      │ ╰─ jne    loop
      │    ret
ret:  ╰──➤ xor    eax, eax
           ret
```
{{% /tab %}}
{{< /tabs >}}


**GCC Runtime:** 0.47s 🦓

**GCC Bitrate**: 1.98GiB/s

**Clang Runtime:** 0.25s 🐆

**Clang Bitrate**: 3.72GiB/s

Sweet, well this is just as fast as our best (cmov) version from the previous
post, and clang's output is almost twice as fast as our previous best version.

Something really weird is going on with the difference between GCC and
clang. How big can the performance difference between these lines really be?

```
movzx  ecx, byte ptr [rdi]
movsx  rax, byte ptr [rdx-1]
```

I think I can get gcc to generate a version with movzx (move and zero-extend)
instead of movsx (move and sign-extend), by using an unsigned integer type
as the instruction, so `uint8_t` or `unsigned char`, instead of `char`.
Let's give it a go:


{{< tabs groupId="arr-lookup-2" >}}
{{% tab name="C" %}}
```C
#include <stdbool.h>
#include <stdint.h>

static
int to_add[256] = {
  ['s'] = 1,
  ['p'] = -1,
};

int run_switches(const uint8_t *input) {
  int res = 0;
  while (true) {
    uint8_t c = *input++;
    if (c == '\0') {
      return res;
    } else {
      res += to_add[(int) c];
    }
  }
}
```
{{% /tab %}}

{{% tab name="gcc asm" %}}
```asm
# objdump -Mintel -d --no-addresses --no-show-raw-insn --visualize-jumps loop-3-gcc.c.o

run_switches:
               movzx  eax, byte ptr [rdi]
               lea    rdx, [rdi+0x1]
               xor    ecx, ecx
               lea    rsi, [rip+0x0]
               test   al,  al
        ╭───── je     ret
        │      nop    dword ptr [rax+0x0]
loop:   │  ╭─➤ movzx  eax, al
        │  │   add    rdx, 1
        │  │   add    ecx, dword ptr [rsi+rax*4]
        │  │   movzx  eax, byte ptr [rdx-1]
        │  │   test   al,  al
        │  ╰── jne    loop
ret:    ╰────➤ mov    eax,ecx
               ret
```
{{% /tab %}}

{{% tab name="clang asm" %}}
```asm
# objdump -Mintel -d --no-addresses --no-show-raw-insn --visualize-jumps loop-3-clang.c.o

run_switches:
               mov    cl,  byte ptr [rdi]
               test   cl,  cl
        ╭───── je     ret
        │      add    rdi, 1
        │      xor    eax, eax
        │      lea    rdx, [rip+0x0]
        │      cs nop word ptr [rax+rax*1+0x0]
        │      nop    dword ptr [rax]
loop:   │  ╭─➤ movzx  ecx, cl
        │  │   add    eax, dword ptr [rdx+rcx*4]
        │  │   movzx  ecx, byte ptr [rdi]
        │  │   add    rdi, 1
        │  │   test   cl,  cl
        │  ╰── jne    loop
        │      ret
ret:    ╰────➤ xor    eax,eax
               ret
```
{{% /tab %}}
{{< /tabs >}}

Well that did the trick, so is it faster?

**GCC Runtime:** 0.47s 🦓

**GCC Bitrate**: 1.98GiB/s

**Clang Runtime:** 0.25s 🐆

**Clang Bitrate**: 3.72GiB/s

Nope. It's exactly the same. Okay, so what's the difference between:

{{< tabs groupId="arr-lookup-2-diff" >}}
{{% tab name="gcc asm" %}}
```asm
loop: ╭─➤ movzx  eax, al
      │   add    rdx, 1
      │   add    ecx, dword ptr [rsi+rax*4]
      │   movzx  eax, byte ptr [rdx-1]
      │   test   al,  al
      ╰── jne    loop
```
{{% /tab %}}

{{% tab name="clang asm" %}}
```asm
loop: ╭─➤ movzx  ecx, cl
      │   add    eax, dword ptr [rdx+rcx*4]
      │   movzx  ecx, byte ptr [rdi]
      │   add    rdi, 1
      │   test   cl,  cl
      ╰── jne    loop
```
{{% /tab %}}
{{< /tabs >}}

Three things:
* Order
* `[rdi]` vs `[rdx - 1]`
* Register choices

## Skipping an extend

Both compilers produced a movzx (move and zero-extend) instruction,
to turn the character of the input into an int.

I don't think this instruction is necessary on either compiler, as
32-bit operations implicitly zero-extend on an x86_64, and I can't
see any code paths where the upper bits would have been set.

That said, some vague intuition tells me we can get rid of this instruction
on the C side by changing

```c
uint8_t c = *input++;
```

to

```c
uint32_t c = *input++;
```

Thus removing all identification of this as an 8-bit value that needs zero-
extending.

{{< tabs groupId="avoid-ze" >}}
{{% tab name="gcc asm" %}}
```asm
loop: /-> add    rdx, 1
      |   add    ecx, dword ptr [rsi+rax*4]
      |   movzx  eax, byte ptr [rdx-1]
      |   test   rax, rax
      \-- jne    loop

```
{{% /tab %}}

{{% tab name="clang asm" %}}
```asm
loop: /-> movzx  ecx,cl
      |   add    eax,dword ptr [rdx+rcx*4]
      |   movzx  ecx,byte ptr [rdi]
      |   add    rdi,0x1
      |   test   cl,cl
      \-- jne    <run_switches+0x20>
```
{{% /tab %}}
{{< /tabs >}}

Well it... worked for GCC.

One less instruction, without dropping down to assembly. Pretty good, huh?

**GCC Runtime:** 0.47s 🦓

**GCC Bitrate**: 1.98GiB/s

Er... that did nothing.

## The asm rewrite

Let's rewrite this in assembly to find out what's going on.

No you can't really reliably reassemble disassembled code...
It's best to rewrite it.

```asm
.text
run_switches:
          xor   eax, eax                   # res = 0
loop:                                      # while (true) {
  /---->  movsx rcx, byte ptr [rdi]        #   char c = *input
  |       test  rcx, rcx                   #   if (c == '\0')
  |  /--  je    ret                        #     return
  |  |    add   eax, dword ptr [arr+rcx*4] #   res += arr[c]
  |  |    inc   rdi                        #   input++
  \--|--  jmp   loop                       # }
ret: \->  ret

.data
arr:
        .fill 'p', 4, 0
        .long -1, 0, 0, 1
        .fill (256 - 's'), 4, 0
```

This is my first pass, and it's a very literal translation of the C version.
There's even two jumps (an extra unconditional one) in the hotpath compared to
GCC and clang's output. So, how does it fare?

**Runtime:** 0.24s 🐆

**Bitrate**: 3.88GiB/s

Well, it... beat both GCC and clang, on the first attempt. I would have expected
it to be harder to write faster code than modern compilers, with their plethora
of optimization passes, yet here we are...

## Tripping over encoding

GCC's output loads the array's address into a register, and uses that to
reference the array in the hotpath:

```asm
lea    rsi, [add_arr]
...
add    ecx, dword ptr [rsi+rax*4]
```

Well, this seems redundant, so we should be able to get rid of the extra `lea`
instruction by using the array's address directly.

```asm
add    ecx, dword ptr [arr+rax*4]
```

We've removed one instruction (`lea`) from the non-hotpath, right? That's true,
but if we look at the instruction encodings side-by-side, we see that we've
increased the number of bytes needed to encode the `add`, and the `add` is the
instruction in the hotpath.

{{< tabs groupId="encodings" >}}
{{% tab name="before" %}}
```asm
48 8d 34 25 00 00 00    lea    rsi,ds:0x0
...
03 0c 86                add    ecx,DWORD PTR [rsi+rax*4]
```
{{% /tab %}}

{{% tab name="after" %}}
```asm

...
03 0c 85 00 00 00 00    add    ecx,DWORD PTR [rax*4+0]
```
{{% /tab %}}
{{< /tabs >}}

I guess loading the array's base address into rsi is a nice optimization to
have, if you have registers to spare. Let's ignore this for now.

## Fixing GCC's code

This doesn't help us debug GCC's output though... So let's adopt GCC's output
into our assembly file, and start tweaking stuff.

The code can be found [here](https://github.com/414owen/blog-code/blob/master/02-twice-as-fast-as-asm/loop-6.x64.s)

If we assemble this version and compare it to GCC's output, we see a slight
difference.
